{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avalanche Prediction via Machine Learning (so I begin...)\n",
    "\n",
    "## Introduction\n",
    "This is the first step of what I hope will be many in utilizing machine learning techniques to predict avalanche phenomena. In this notebook, I'm aim to present a little example of this by using a support vector machine (SVM) and three freely available datasets to predict avalanche detection (you'll see here soon why I wrote avalanche *detection* and not avalanche *occurrence*). Let's begin with the most important part, the data:.\n",
    "\n",
    "## The datasets\n",
    "For simplicities sake, I only looked at data from the current winter, Oct 1, 2019 to April 4, 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "start_time = pd.Timestamp(\"2019-10-01 00:00:00\")\n",
    "end_time = pd.Timestamp(\"2020-04-04 00:00:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Avalanches: \n",
    "The first dataset I got my hands on was from the Utah Avalanche Center. They have a [database of avalanches](https://utahavalanchecenter.org/avalanches) that goes all the way back to 1914. My mom's dad was 3 at the time. It's awesome. There's lots of great info in there. But alas, right off the bat we have a huge caveat to add to any conclusion that can be drawn with this data: this isn't a table of avalanches; it's a table of __*avalanche reportings*__. Thus, basically any model that uses this data is only going to be able to predict if somebody will __*see and report*__ an avalanche not that an avalanche occurs.\n",
    "\n",
    "One idea for the future would be to use satellite imagery to detect avalanches. If the imagery is high res enough, a machine could be trained to detect them and provide coordinates. This would also provide the benefit of allowing both the slope angle and elevation to be calculated. Unfortunately, some days it's cloudy (NO WAY!), so this wouldn't provide comprehensive avalanche detection. Just spitballing.\n",
    "\n",
    "For simplicity, I subsetted the data to only include avalanche reports from the \"Salt Lake\" region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "avy_data = pd.read_csv(\"avalanches.csv\", parse_dates=['Date'])\n",
    "avy_data = avy_data.rename(columns={'Date': 'date'})\n",
    "\n",
    "# only Salt Lake\n",
    "avy_data = avy_data.loc[avy_data.Region == 'Salt Lake']\n",
    "\n",
    "# subset select time period\n",
    "avy_data = avy_data[((avy_data.date >= start_time + pd.Timedelta(days=1)) &\n",
    "                     (avy_data.date <= end_time))]\n",
    "\n",
    "# count avalanches reported per day\n",
    "avy_count = avy_data.date.value_counts()\n",
    "\n",
    "# fill in missing datetimes\n",
    "idx = pd.date_range(start_time + pd.Timedelta(days=1), end_time)\n",
    "avy_count = avy_count.reindex(idx)\n",
    "avy_count = avy_count.fillna(0)\n",
    "\n",
    "# convert to any avalanche seen vs no avalanches seen\n",
    "avy = avy_count.astype(bool).astype(int)\n",
    "avy = avy.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Snow data:\n",
    "SNOTEL has a pretty easy-to-use [web interface](http://wcc.sc.egov.usda.gov/reportGenerator/) to get information from their SNOTEL sites. For the current exploratory mission, I just used data from SITE 628 - \"MILL-D NORTH\". At this site, there are four measurements recorded: snow-water equivalent, snow depth, precipitation accumulation, and temperature. I didn't use precipitation accumulation because I don't really know what that means in this context (I haven't had time to figure out yet (April 17, 2020)). These measurements are taken hourly. There are a few lapses (I wanna say 300 bad recordings out of 13,000). I just interpolated to fill them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snotel = pd.read_csv(\"snotel.csv\", parse_dates=['date'])\n",
    "\n",
    "# subset for select time period\n",
    "snotel = snotel[(snotel.date > start_time + pd.Timedelta(hours=9)) &\n",
    "                (snotel.date <= end_time + pd.Timedelta(hours=9))]\n",
    "\n",
    "# drop pa_in cause I found some mismeasurements and what is this anyway?\n",
    "snotel = snotel.drop(['pa_in'], axis=1)\n",
    "\n",
    "# fill in missing dates\n",
    "snotel = snotel.set_index(['date'])\n",
    "idx = pd.date_range(snotel.index.min(), snotel.index.max(), freq='H')\n",
    "snotel = snotel.reindex(idx)\n",
    "\n",
    "# find nulls and interpolate\n",
    "snotel = snotel.interpolate(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Wind and Sky Cover data:\n",
    "This data was surprisingly hard to find. I would think that there would be publicly available, hourly recordings of wind speed and direction recorded at elevations above 10,000 ft in the Wasatch but I couldn't find any (like at a resort's weather station?). I figured finding sky cover data would be more difficult. \n",
    "\n",
    "I found hourly recordings of both variables at a [NOAA website](https://www.ncdc.noaa.gov/cdo-web/). They were recorded at the Salt Lake Airport. The wind speed is just plain ole wind speed but the sky cover measurement is more abstruse. I'm gonna pull a tenured professor move and encourage the interested reader to read the [documentation](https://www1.ncdc.noaa.gov/pub/data/cdo/documentation/NORMAL_HLY_documentation.pdf). I dropped all other remaining variables.\n",
    "\n",
    "I'm not going to bore you by describing how I cleaned the data/dealt with it's eccentricities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (19,25,26,27,29,30,31,33,35,36,41,42,43,49,51,52,58,59,69,71,73,75,76,88,89,110) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# convert feet or inches to inches\n",
    "def conv_to_in(x):\n",
    "    if x.endswith(\"'\"):\n",
    "        return float(x[:-1]) * 12\n",
    "    if x.endswith('\"'):\n",
    "        return float(x[:-1])\n",
    "\n",
    "\n",
    "# convert feet or inches to feet\n",
    "def conv_to_ft(x):\n",
    "    x = x.replace(',', '')\n",
    "    if x.endswith(\"'\"):\n",
    "        return float(x[:-1])\n",
    "    if x.endswith('\"'):\n",
    "        return float(x[:-1]) / 12\n",
    "\n",
    "\n",
    "# parse sky_unparsed column\n",
    "def parse_sky(x):\n",
    "    if pd.isna(x):\n",
    "        return x\n",
    "    else:\n",
    "        pieces = x.split()\n",
    "\n",
    "        if len(pieces) == 1:\n",
    "            return 0\n",
    "        else:\n",
    "            highest = pieces[-2]\n",
    "            return int(highest[4:6])\n",
    "\n",
    "\n",
    "noaa = pd.read_csv(\"noaa.csv\", parse_dates=['DATE'])\n",
    "\n",
    "# select features to keep and rename\n",
    "noaa = noaa[['DATE', 'HourlySkyConditions', 'HourlyWindSpeed']]\n",
    "noaa = noaa.rename(columns={'DATE': 'date',\n",
    "                            'HourlySkyConditions': 'sky_unparsed',\n",
    "                            'HourlyWindSpeed': 'windspeed'})\n",
    "\n",
    "# subset for select time period\n",
    "noaa = noaa[(noaa.date > start_time + pd.Timedelta(hours=9)) &\n",
    "            (noaa.date <= end_time + pd.Timedelta(hours=9))]\n",
    "\n",
    "# get rid of rows that don't make sense (are just a number)\n",
    "noaa = noaa.loc[~(noaa['sky_unparsed'].apply(lambda x: str(x).isdigit()))]\n",
    "\n",
    "# parse sky_unparsed column\n",
    "noaa['sky'] = noaa['sky_unparsed'].apply(lambda x: parse_sky(x))\n",
    "noaa = noaa.drop(['sky_unparsed'], axis=1)\n",
    "\n",
    "# interpolate the missing data\n",
    "noaa = noaa.interpolate(axis=0)\n",
    "\n",
    "# round datetimes\n",
    "noaa['date'] = noaa['date'].apply(lambda x: x.round('60min'))\n",
    "\n",
    "# remove rows with duplicate datetimes\n",
    "noaa = noaa.drop_duplicates('date')\n",
    "\n",
    "# fill in missing datetimes\n",
    "noaa = noaa.set_index(['date'])\n",
    "idx = pd.date_range(noaa.index.min(), noaa.index.max(), freq='H')\n",
    "noaa = noaa.reindex(idx)\n",
    "noaa = noaa.interpolate(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
